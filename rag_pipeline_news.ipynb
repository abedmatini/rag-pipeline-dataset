{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a275392",
   "metadata": {},
   "source": [
    "\n",
    "## 1 Importing the necessary libraries\n",
    "\n",
    "Run the cell below to import the necessary libraries for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868905ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    retrieve, \n",
    "    pprint, \n",
    "    generate_with_single_input, \n",
    "    read_dataframe, \n",
    "    display_widget\n",
    ")\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c33ce1",
   "metadata": {},
   "source": [
    "## 2 - Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa472150",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA = read_dataframe(\"news_data_dedup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49079c91",
   "metadata": {},
   "source": [
    "Let's check the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66057196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(NEWS_DATA[9:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad27afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_news(indices):\n",
    "    \"\"\"\n",
    "    Retrieves elements from a dataset based on specified indices.\n",
    "\n",
    "    Parameters:\n",
    "    indices (list of int): A list containing the indices of the desired elements in the dataset.\n",
    "    dataset (list or sequence): The dataset from which elements are to be retrieved. It should support indexing.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of elements from the dataset corresponding to the indices provided in list_of_indices.\n",
    "    \"\"\"\n",
    "     \n",
    "    output = [NEWS_DATA[index] for index in indices]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad49149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching some indices\n",
    "indices = [3, 6, 9]\n",
    "pprint(query_news(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90186e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the retrieve function!\n",
    "indices = retrieve(\"Regulations about Autopilot\", top_k = 1)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's query the corresponding news_\n",
    "retrieved_documents = query_news(indices)\n",
    "pprint(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def get_relevant_data(query: str, top_k: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve and return the top relevant data items based on a given query.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Retrieves the indices of the top 'k' relevant items from a dataset based on the provided `query`.\n",
    "    2. Fetches the corresponding data for these indices from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query string used to find relevant items.\n",
    "    - top_k (int, optional): The number of top items to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - list[dict]: A list of dictionaries containing the data associated \n",
    "      with the top relevant items.\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve the indices of the top_k relevant items given the query\n",
    "    relevant_indices = retrieve(query, top_k)\n",
    "\n",
    "    # Obtain the data related to the items using the indices from the previous step\n",
    "    relevant_data = query_news(relevant_indices)\n",
    "\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Greatest storms in the US\"\n",
    "relevant_data = get_relevant_data(query, top_k = 1)\n",
    "pprint(relevant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to perform several tests on your function. If you receive \"All test passed!\" it means that your solution will likely pass the autograder too.\n",
    "unittests.test_get_relevant_data(get_relevant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab4851",
   "metadata": {},
   "source": [
    "## Formatting the relevant Data\n",
    "\n",
    " It’s recommended to use double quotes for strings and single quotes for dictionary keys (for example, data['title']). Here’s one way you could format it:\n",
    "\n",
    "f\"\"\"\n",
    "Title: {news_1_title}, Description: {news_1_description}, Published at: {news_1_published_date}\\nURL: {news_1_URL}\n",
    "Title: {news_2_title}, Description: {news_2_description}, Published at: {news_2_published_date}\\nURL: {news_2_URL}\n",
    "...\n",
    "Title: {news_k_title}, Description: {news_k_description}, Published at: {news_k_published_date}\\nURL: {news_k_URL}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51977f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL\n",
    "\n",
    "def format_relevant_data(relevant_data):\n",
    "    \"\"\"\n",
    "    Retrieves the top_k most relevant documents based on a given query and constructs an augmented prompt for a RAG system.\n",
    "\n",
    "    Parameters:\n",
    "    relevant_data (list): A list with relevant data.\n",
    "\n",
    "    Returns:\n",
    "    str: An augmented prompt with the top_k relevant documents, formatted for use in a Retrieval-Augmented Generation (RAG) system.\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a list so store the formatted documents\n",
    "    formatted_documents = []\n",
    "    \n",
    "    # Iterates over each relevant document.\n",
    "    for document in relevant_data:\n",
    "\n",
    "        # Formats each document into a structured layout string. Remember that each document is in one different line. So you should add a new line character after each document added.\n",
    "        formatted_document = f\"Title: {document['title']}, Description: {document['description']}, Published at: {document['published_at']}\\nURL: {document['url']}\"\n",
    "        \n",
    "        # Append the formatted document string to the formatted_documents list\n",
    "        formatted_documents.append(formatted_document)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Returns the final augmented prompt string.\n",
    "\n",
    "    return \"\\n\".join(formatted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ee234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f03c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a15486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07460e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = NEWS_DATA[4:8]\n",
    "print(format_relevant_data(example_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function!\n",
    "unittests.test_format_relevant_data(format_relevant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675cc3c",
   "metadata": {},
   "source": [
    "## 3.5 Generate the final prompt\n",
    "\n",
    "The next function is given to you. It will generate the final prompt, integrating it with the query. Feel free to change the prompt and experiment how different prompts impact the final result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE CELL\n",
    "\n",
    "def generate_final_prompt(query, top_k=5, use_rag=True, prompt=None):\n",
    "    \"\"\"\n",
    "    Generates a final prompt based on a user query, optionally incorporating relevant data using retrieval-augmented generation (RAG).\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query for which the prompt is to be generated.\n",
    "        top_k (int, optional): The number of top relevant data pieces to retrieve and incorporate. Default is 5.\n",
    "        use_rag (bool, optional): A flag indicating whether to use retrieval-augmented generation (RAG)\n",
    "                                  by including relevant data in the prompt. Default is True.\n",
    "        prompt (str, optional): A template string for the prompt. It can contain placeholders {query} and {documents}\n",
    "                                for formatting with the query and formatted relevant data, respectively.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt, either consisting solely of the query or expanded with relevant data\n",
    "             formatted for additional context.\n",
    "    \"\"\"\n",
    "    # If RAG is not being used, format the prompt with just the query or return the query directly\n",
    "    if not use_rag:\n",
    "        return query\n",
    "\n",
    "    # Retrieve the top_k relevant data pieces based on the query\n",
    "    relevant_data = get_relevant_data(query, top_k=top_k)\n",
    "\n",
    "    # Format the retrieved relevant data\n",
    "    retrieve_data_formatted = format_relevant_data(relevant_data)\n",
    "\n",
    "    # If no custom prompt is provided, use the default prompt template\n",
    "    if prompt is None:\n",
    "        prompt = (\n",
    "            f\"Answer the user query below. There will be provided additional information for you to compose your answer. \"\n",
    "            f\"The relevant information provided is from 2024 and it should be added as your overall knowledge to answer the query, \"\n",
    "            f\"you should not rely only on this information to answer the query, but add it to your overall knowledge.\"\n",
    "            f\"Query: {query}\\n\"\n",
    "            f\"2024 News: {retrieve_data_formatted}\"\n",
    "        )\n",
    "    else:\n",
    "        # If a custom prompt is provided, format it with the query and formatted relevant data\n",
    "        prompt = prompt.format(query=query, documents=retrieve_data_formatted)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37590e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_final_prompt(\"Tell me about the US GDP in the past 3 years.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_final_prompt(\"Tell me about paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514642d0",
   "metadata": {},
   "source": [
    "## 3.6 LLM call\n",
    "Now let's integrate the function above to feed an LLM. Its parameters are:\n",
    "\n",
    "query: the query to be passed to the LLM.\n",
    "use_rag: a boolean telling whether using RAG or not. This parameter will help you compare queries using a RAG system and not using it.\n",
    "model: the model to be used. You might change it, but the standard is the Llama 3 Billion parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(query, top_k = 5, use_rag = True, prompt = None):\n",
    "    \"\"\"\n",
    "    Calls the LLM to generate a response based on a query, optionally using retrieval-augmented generation.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query that will be processed by the language model.\n",
    "        use_rag (bool, optional): A flag that indicates whether to use retrieval-augmented generation by \n",
    "                                  incorporating relevant documents into the prompt. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response generated by the language model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Get the prompt with the query + relevant documents\n",
    "    prompt = generate_final_prompt(query, top_k, use_rag, prompt)\n",
    "\n",
    "    # Call the LLM\n",
    "    generated_response = generate_with_single_input(prompt)\n",
    "\n",
    "    # Get the content\n",
    "    generated_message = generated_response['content']\n",
    "    \n",
    "    return generated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the US GDP in the past 3 years.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_call(query, use_rag = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_call(query, use_rag = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc67894",
   "metadata": {},
   "source": [
    "## 4 - Experimenting with your RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dec405",
   "metadata": {},
   "source": [
    "Now you can experiment with your own queries to see the system in action! You can write any query, and it will display answers both with and without RAG. Keep in mind that the dataset you're working with is related to news data from 2024, so not all queries will be effective in demonstrating the framework. Some example queries you might try include:\n",
    "\n",
    "What were the most important events of the past year?\n",
    "How is global warming progressing in 2024?\n",
    "Tell me about the most recent advances in AI.\n",
    "Give me the most important facts from past year.\n",
    "You can also specify a layout for the augmented prompt that includes placeholders for {query} and {documents} to indicate where they should be inserted within your prompt structure. For example:\n",
    "\n",
    "This is the query: {query}\n",
    "These are the documents: {documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_widget(llm_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a985c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
